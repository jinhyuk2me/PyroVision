# test_video_inference.py
import cv2
import time
import numpy as np
from detector.tflite import TFLiteWorker
from configs.get_cfg import get_cfg

def test_video_inference(video_path, model_path, labels_path, 
                         use_npu=True, save_output=True):
    """
    ÎπÑÎîîÏò§ ÌååÏùº Î°úÎìú ‚Üí ÌîÑÎ†àÏûÑÎ≥Ñ Ï∂îÎ°† ‚Üí ÏãúÍ∞ÅÌôî ‚Üí Ï†ÄÏû•
    
    Args:
        video_path: ÏûÖÎ†• ÎπÑÎîîÏò§ Í≤ΩÎ°ú (Ïòà: 'output.mp4')
        model_path: TFLite Î™®Îç∏ Í≤ΩÎ°ú
        labels_path: ÎùºÎ≤® ÌååÏùº Í≤ΩÎ°ú
        use_npu: NPU ÏÇ¨Ïö© Ïó¨Î∂Ä
        save_output: Í≤∞Í≥º ÎπÑÎîîÏò§ Ï†ÄÏû• Ïó¨Î∂Ä
    """
    
    # ===== 1. ÎπÑÎîîÏò§ Î°úÎìú =====
    pipeline = (
        "filesrc location=output.mp4 ! "
        "qtdemux name=demux "
        "demux.video_0 ! queue ! h264parse ! avdec_h264 ! "
        "videoconvert ! video/x-raw,format=BGR ! "
        "appsink drop=true max-buffers=1"
    )

    cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)

    # cap = cv2.VideoCapture('output.mp4', cv2.CAP_FFMPEG)
    
    if not cap.isOpened():
        print(f"‚ùå Failed to open video: {video_path}")
        return
    
    # ÎπÑÎîîÏò§ Ï†ïÎ≥¥
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"üìπ Video Info:")
    print(f"   Resolution: {width}x{height}")
    print(f"   FPS: {fps}")
    print(f"   Total frames: {total_frames}")
    
    # ===== 2. Î™®Îç∏ Î°úÎìú =====
    print(f"\nü§ñ Loading model...")
    
    # TFLiteWorker Ï¥àÍ∏∞Ìôî (Î≤ÑÌçº ÏóÜÏù¥ ÏßÅÏ†ë ÏÇ¨Ïö©)
    cfg = get_cfg()
    worker = TFLiteWorker(
        model_path=model_path,
        labels_path=labels_path,
        input_buf=None,  # ÎπÑÎîîÏò§ ÌååÏùº ÏÇ¨Ïö© Ïãú Î∂àÌïÑÏöî
        output_buf=None,
        use_npu=use_npu,
        delegate_lib=cfg.get('DELEGATE', '/usr/lib/libvx_delegate.so'),
        cpu_threads=2,
        target_fps=0,  # Ï†úÌïú ÏóÜÏùå
        target_res=(1280, 720),  # Ï∂úÎ†• Ìï¥ÏÉÅÎèÑ
        name="VideoTest"
    )
    
    print(f"‚úÖ Model loaded (accel={worker.accel})")
    
    # ===== 3. Ï∂úÎ†• ÎπÑÎîîÏò§ ÏÑ§Ï†ï =====
    output_path = None
    video_writer = None
    
    if save_output:
        output_path = video_path.replace('.mp4', '_detected.mp4')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(
            output_path, fourcc, fps, 
            (worker.target_res[0] if worker.target_res else width,
             worker.target_res[1] if worker.target_res else height)
        )
        print(f"üíæ Output will be saved to: {output_path}")
    
    # ===== 4. ÌîÑÎ†àÏûÑÎ≥Ñ Ï∂îÎ°† =====
    print(f"\nüî• Processing frames...\n")
    
    frame_count = 0
    total_time = 0
    detection_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        
        # Ï∂îÎ°† Ïã§Ìñâ
        start_time = time.time()
        scores, boxes_xyxy, classes = worker._infer_once(frame)
        inference_time = (time.time() - start_time) * 1000  # ms
        total_time += inference_time
        
        # ÌôîÏ†ê ÌÉêÏßÄ Ïó¨Î∂Ä (class 0 or 1)
        has_fire = any((cls in [0, 1] and score > 0.25) 
                       for cls, score in zip(classes, scores))
        
        if has_fire:
            detection_count += 1
        
        # Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞
        vis_frame = frame.copy()
        from detector.tflite import _draw_boxes
        vis_frame = _draw_boxes(
            vis_frame, boxes_xyxy, classes, scores, 
            worker.labels, thr=0.25
        )
        
        # target_resÎ°ú Î¶¨ÏÇ¨Ïù¥Ï¶à
        if worker.target_res:
            vis_frame = cv2.resize(
                vis_frame, 
                (worker.target_res[0], worker.target_res[1]),
                interpolation=cv2.INTER_AREA
            )
        
        # ÌôîÎ©¥ ÌëúÏãú
        display_frame = vis_frame.copy()
        
        # ÏÉÅÌÉú Ï†ïÎ≥¥ Ïò§Î≤ÑÎ†àÏù¥
        status_text = f"Frame: {frame_count}/{total_frames}"
        fire_text = "FIRE DETECTED!" if has_fire else "No Fire"
        fps_text = f"FPS: {1000/inference_time:.1f} ({inference_time:.1f}ms)"
        
        cv2.putText(display_frame, status_text, (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(display_frame, fire_text, (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, 
                    (0, 0, 255) if has_fire else (0, 255, 0), 2)
        cv2.putText(display_frame, fps_text, (10, 90),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        cv2.imshow('Fire Detection', display_frame)
        
        # ÎπÑÎîîÏò§ Ï†ÄÏû•
        if video_writer:
            video_writer.write(vis_frame)
        
        # ÏßÑÌñâÎ•† Ï∂úÎ†•
        if frame_count % 30 == 0:
            progress = frame_count / total_frames * 100
            avg_fps = 1000 / (total_time / frame_count)
            print(f"Progress: {progress:.1f}% | "
                  f"Avg FPS: {avg_fps:.1f} | "
                  f"Detections: {detection_count}")
        
        # 'q' ÌÇ§Î°ú Ï§ëÎã®
        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("\n‚ö†Ô∏è  Stopped by user")
            break
    
    # ===== 5. Ï†ïÎ¶¨ =====
    cap.release()
    if video_writer:
        video_writer.release()
    cv2.destroyAllWindows()
    
    # ===== 6. Í≤∞Í≥º Ï∂úÎ†• =====
    print(f"\nüìä Results:")
    print(f"   Processed frames: {frame_count}/{total_frames}")
    print(f"   Total detection count: {detection_count}")
    print(f"   Detection rate: {detection_count/frame_count*100:.1f}%")
    print(f"   Average inference time: {total_time/frame_count:.1f} ms")
    print(f"   Average FPS: {1000/(total_time/frame_count):.1f}")
    
    if save_output and output_path:
        print(f"\n‚úÖ Output saved: {output_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Video Fire Detection Test')
    parser.add_argument('video', type=str, help='Input video path (e.g., output.mp4)')
    parser.add_argument('--model', type=str, 
                        default='/root/vision-ai-nxp/model/best_full_integer_quant.tflite',
                        help='TFLite model path')
    parser.add_argument('--labels', type=str,
                        default='/root/vision-ai-nxp/model/labels.txt',
                        help='Labels file path')
    parser.add_argument('--cpu', action='store_true',
                        help='Use CPU instead of NPU')
    parser.add_argument('--no-save', action='store_true',
                        help='Do not save output video')
    
    args = parser.parse_args()
    
    test_video_inference(
        video_path=args.video,
        model_path=args.model,
        labels_path=args.labels,
        use_npu=not args.cpu,
        save_output=not args.no_save
    )